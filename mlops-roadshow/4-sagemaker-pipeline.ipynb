{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b55228",
   "metadata": {},
   "source": [
    "# MLOps Manual to Repeatable Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9991b4",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "- [Training pipeline with SageMaker Pipelines](#Training-pipeline-with-SageMaker-Pipelines)\n",
    "    - [Pipeline inputs](#Pipeline-inputs)\n",
    "    - [SageMaker Processing step](#SageMaker-Processing-step)\n",
    "    - [SageMaker Training step](#SageMaker-Training-step)\n",
    "    - [Model evaluation step](#Model-evaluation-step)\n",
    "    - [Register model in Model Registry step](#Register-model-in-Model-Registry-step)\n",
    "    - [Assemble the training pipeline](#Assemble-the-training-pipeline)\n",
    "    - [Execute the training pipeline](#Execute-the-training-pipeline)\n",
    "- [Deployment pipeline with SageMaker Pipelines](#Deployment-pipeline-with-SageMaker-Pipelines)\n",
    "    - [Assemble the deployment pipeline](#Assemble-the-deployment-pipeline)\n",
    "    - [Execute the deployment pipeline](#Execute-the-deployment-pipeline)\n",
    "    - [Test the SageMaker endpoint](#Test-the-SageMaker-endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c955aa1",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a698b834",
   "metadata": {},
   "source": [
    "This is our fourth notebook which will explore the orchestration stage of ML workflow.\n",
    "\n",
    "Here, we will put on the hat of a `DevOps/MLOps Engineer` and perform the task of orchestration which includes building pipeline steps that include all the previous notebooks components into one singular entity. This pipeline entity accomplishes a repeatable and reliable orchestration of each step in the ML workflow.\n",
    "\n",
    "For this task we will be using Amazon SageMaker Pipeline capabilities. We will be creating two SageMaker Pipelines, one for model training and one for model deployment.\n",
    "\n",
    "<div>\n",
    "<img src=\"./pipeline_scripts/images/training_and_deployment_pipelines.png\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c3c66",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de783141",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf9398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing imports\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "\n",
    "# SageMaker Pipeline imports\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep, TransformStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "# Other imports\n",
    "import json\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.tuner import IntegerParameter, HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "\n",
    "# To test the endpoint once it's deployed\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer, CSVDeserializer\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "from sagemaker.model_metrics import ModelMetrics, MetricsSource\n",
    "import pandas as pd\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f7796",
   "metadata": {},
   "source": [
    "**Session variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lambda_iam_role(role_name):\n",
    "    iam = boto3.client(\"iam\")\n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps({\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"lambda.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }),\n",
    "            Description='Role for Lambda to call SageMaker'\n",
    "        )\n",
    "\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "        )\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    "            RoleName=role_name\n",
    "        )\n",
    "\n",
    "        return role_arn\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f'Using ARN from existing role: {role_name}')\n",
    "        response = iam.get_role(RoleName=role_name)\n",
    "        print(\"Done\")\n",
    "        return response['Role']['Arn']\n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps({\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"lambda.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }),\n",
    "            Description='Role for Lambda to call SageMaker'\n",
    "        )\n",
    "\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "        )\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    "            RoleName=role_name\n",
    "        )\n",
    "        print(\"Done\")\n",
    "\n",
    "        return role_arn\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f'Using ARN from existing role: {role_name}')\n",
    "        response = iam.get_role(RoleName=role_name)\n",
    "        print(\"Done\")\n",
    "        return response['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb244ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful SageMaker variables\n",
    "session = PipelineSession()\n",
    "bucket = session.default_bucket()\n",
    "role_arn= sagemaker.get_execution_role()\n",
    "region = session.boto_region_name\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "aws_account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "lambda_role = create_lambda_iam_role('LambdaSageMakerExecutionRole')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f885a04",
   "metadata": {},
   "source": [
    "## Training pipeline with SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be5471",
   "metadata": {},
   "source": [
    "An Amazon [SageMaker Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) pipeline is a series of interconnected steps that is defined by a JSON pipeline definition. This pipeline definition encodes a pipeline using a directed acyclic graph (DAG). This DAG gives information on the requirements for and relationships between each step of your pipeline. The structure of a pipeline's DAG is determined by the data dependencies between steps. These data dependencies are created when the properties of a step's output are passed as the input to another step. The following image is a pipeline DAG that we'll be creating for our training pipeline:\n",
    "\n",
    "![](./pipeline_scripts/images/sagemaker-pipelines-dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c57073-02f7-4fa1-aea8-f9bfb0231a1f",
   "metadata": {},
   "source": [
    "You can also include other steps to your pipeline, for example for performing Hyperparameter Optimization (HPO) on your training pipeline. [Pipeline Steps Types](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#build-and-manage-steps-types) has a list of all posible pipeline step types that you can use to build your pipeline and [this workshop](https://aws.amazon.com/getting-started/hands-on/machine-learning-tutorial-mlops-automate-ml-workflows/#) explains how to build a SageMaker pipeline with steps for data bias check and model explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cefc9b",
   "metadata": {},
   "source": [
    "#### Pipeline inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135d550",
   "metadata": {},
   "source": [
    "You can give a pipeline inputs to make it reusable (you'll be able to override these inputs upon executing the pipeline later in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_count = ParameterInteger(\n",
    "    name='ProcessingInstanceCount',\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name='ProcessingInstanceType',\n",
    "    default_value='ml.m5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb440646",
   "metadata": {},
   "source": [
    "#### SageMaker Processing step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961856d",
   "metadata": {},
   "source": [
    "This should look very similar to the SageMaker Training job you did in notebook 2. The only new line of code is the `ProcessingStep` line at the bottom of the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role_arn,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name='preprocess-data',\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "preprocess_dataset_step = ProcessingStep(\n",
    "    name='PreprocessData',\n",
    "    code='./pipeline_scripts/preprocessing.py',\n",
    "    processor=preprocess_data_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=raw_s3,\n",
    "            destination='/opt/ml/processing/input',\n",
    "            s3_data_distribution_type='ShardedByS3Key'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='train',\n",
    "            destination=f'{output_path}/train',\n",
    "            source='/opt/ml/processing/train'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='validation',\n",
    "            destination=f'{output_path}/validation',\n",
    "            source='/opt/ml/processing/validation'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='test',\n",
    "            destination=f'{output_path}/test',\n",
    "            source='/opt/ml/processing/test'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a13bfa",
   "metadata": {},
   "source": [
    "#### SageMaker Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa727f",
   "metadata": {},
   "source": [
    "This should look very similar to the SageMaker Training job you did in notebook 2. The only new line of code is the `TrainingStep` line at the bottom of the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65af28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned hyperparameters\n",
    "hyperparameters = {\n",
    "    \"max_depth\": \"7\",\n",
    "    \"gamma\": \"2\",\n",
    "    \"alpha\": \"375\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"50\",\n",
    "    \"verbosity\": \"2\",\n",
    "    \"eval_metric\": \"mse\"\n",
    "}\n",
    "\n",
    "train_instance_type = 'ml.c5.xlarge'\n",
    "\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "#xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.5-1\")\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgboost_container, \n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role_arn,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.2xlarge', \n",
    "    volume_size=5, # 5 GB \n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name='TrainModel',\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=preprocess_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'train'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': TrainingInput(\n",
    "            s3_data=preprocess_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'validation'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcdd83e",
   "metadata": {},
   "source": [
    "#### Model evaluation step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abb56a",
   "metadata": {},
   "source": [
    "After the training step in our pipeline, we'll want to then evaluate our model's performance. To do that, we can create a SageMaker Processing Step and pass in some code to do the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd97e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_processor = ScriptProcessor(\n",
    "    image_uri=xgboost_container,\n",
    "    command=[\"python3\"],\n",
    "    role=role_arn,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name='evaluation',\n",
    "    sagemaker_session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where we'll store the model evaluation results so\n",
    "# that other steps can access those results\n",
    "evaluation_report = PropertyFile(\n",
    "    name='EvaluationReport',\n",
    "    output_name='evaluation',\n",
    "    path='evaluation.json',\n",
    ")\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name='EvaluateModel',\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/model',\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=preprocess_dataset_step.properties.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri,\n",
    "            destination='/opt/ml/processing/test',\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='evaluation', source='/opt/ml/processing/evaluation'\n",
    "        ),\n",
    "    ],\n",
    "    code='./pipeline_scripts/evaluation.py',\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd32643",
   "metadata": {},
   "source": [
    "#### Register model in Model Registry step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b43c83",
   "metadata": {},
   "source": [
    "Once we've evaluated the model's peformance, we'll want to register the model in a Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri='{}/evaluation.json'.format(\n",
    "            evaluation_step.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output'][\n",
    "                'S3Uri'\n",
    "            ]\n",
    "        ),\n",
    "        content_type='application/json',\n",
    "    )\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    image_uri=estimator.training_image_uri(),\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    entry_point=estimator.entry_point,\n",
    "    role=role_arn,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "model_registry_args = model.register(\n",
    "    content_types=['text/csv'],\n",
    "    response_types=['application/json'],\n",
    "    inference_instances=['ml.m5.xlarge'],\n",
    "    transform_instances=['ml.m5.xlarge'],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status='PendingManualApproval',\n",
    "    model_metrics=model_metrics\n",
    ")\n",
    "\n",
    "register_step = ModelStep(\n",
    "    name='RegisterModel',\n",
    "    step_args=model_registry_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb436e40",
   "metadata": {},
   "source": [
    "But we'll only want to register the model if its performance meets a predefined threshold that we set. So let's create a Condition Step that says if our model's MSE values is less than 80000000.0, then we'll registery the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a45852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition step for evaluating model quality and branching execution\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path='regression_metrics.mse.value',\n",
    "    ),\n",
    "    right=80000000.0,\n",
    ")\n",
    "condition_step = ConditionStep(\n",
    "    name='CheckEvaluation',\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[register_step],\n",
    "    else_steps=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae7857",
   "metadata": {},
   "source": [
    "#### Assemble the training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d737ad",
   "metadata": {},
   "source": [
    "Though easier to reason with, the parameters and steps don't need to be in order. The pipeline DAG will parse it out properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = 'synthetic-housing-training-pipeline-{}'.format(strftime('%d-%H-%M-%S', gmtime()))\n",
    "pipeline_name = 'synthetic-housing-training-pipeline'\n",
    "step_list = [preprocess_dataset_step,\n",
    "             training_step,\n",
    "             evaluation_step,\n",
    "             condition_step]\n",
    "\n",
    "training_pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        processing_instance_type\n",
    "    ],\n",
    "    steps=step_list\n",
    ")\n",
    "\n",
    "# Note: If an existing pipeline has the same name it will be overwritten.\n",
    "training_pipeline.upsert(role_arn=role_arn)\n",
    "\n",
    "# Viewing the pipeline definition will all the string variables interpolated may help debug pipeline bugs. It is commented out here due to length.\n",
    "#json.loads(training_pipeline.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e5187",
   "metadata": {},
   "source": [
    "#### Execute the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = training_pipeline.start(\n",
    "    parameters = {\n",
    "        'ProcessingInstanceType': 'ml.m5.large'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d7ff1",
   "metadata": {},
   "source": [
    "Check on status of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8bc68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84fabe",
   "metadata": {},
   "source": [
    "## Deployment pipeline with SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9770d04",
   "metadata": {},
   "source": [
    "Now let's create a separate pipeline that will take the model that was registered in Model Registry and deploy it as a SageMaker hosted endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0f831",
   "metadata": {},
   "source": [
    "First we'll specify the input parameters to our deployment pipeline so that we can reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586cb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ParameterString(\n",
    "    name='ModelName',\n",
    "    default_value='my-awesome-model'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e162f",
   "metadata": {},
   "source": [
    "Next, we'll create a Lambda function that will pull the specified model (or latest model) from the Model Registry and deploy as a Sagemaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_name = 'sagemaker-pipelines-deploy-model'\n",
    "\n",
    "lambda_function = Lambda(\n",
    "    function_name=lambda_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    script='./pipeline_scripts/lambda_deploy.py',\n",
    "    handler='lambda_deploy.lambda_handler',\n",
    "    timeout=600,\n",
    "    memory_size=3000,\n",
    ")\n",
    "\n",
    "try:\n",
    "    lambda_function_response = lambda_function.create()\n",
    "    lambda_function_arn = lambda_function_response['FunctionArn']\n",
    "    print(f'Lambda function arn: {lambda_function_arn}')\n",
    "except:\n",
    "    print('Lambda function already exists!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514b2cd",
   "metadata": {},
   "source": [
    "Now we'll create a Lambda step for our pipeline and associate it with the new Lambda function we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary retured by the Lambda function is captured by LambdaOutput, each key in the dictionary corresponds to a\n",
    "# LambdaOutput\n",
    "\n",
    "output_param_1 = LambdaOutput(output_name='statusCode', output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_2 = LambdaOutput(output_name='body', output_type=LambdaOutputTypeEnum.String)\n",
    "\n",
    "deploy_lambda_step = LambdaStep(\n",
    "    name='LambdaStepDeploy',\n",
    "    lambda_func=lambda_function,\n",
    "    inputs={\n",
    "        'region': region,\n",
    "        'aws_account_id': aws_account_id,\n",
    "        'model_package_group_name': model_package_group_name,\n",
    "        'model_name': model_name,\n",
    "        'instance_count': 1,\n",
    "        'role_arn': role_arn\n",
    "    },\n",
    "    outputs=[\n",
    "        output_param_1, \n",
    "        output_param_2\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d029f8c",
   "metadata": {},
   "source": [
    "Excellent, now we just need to assemble the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc014f56",
   "metadata": {},
   "source": [
    "#### Assemble the deployment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = 'synthetic-housing-deployment-pipeline-{}'.format(strftime('%d-%H-%M-%S', gmtime()))\n",
    "pipeline_name = 'synthetic-housing-deployment-pipeline'\n",
    "step_list = [deploy_lambda_step]\n",
    "\n",
    "deployment_pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        model_name\n",
    "    ],\n",
    "    steps=step_list\n",
    ")\n",
    "\n",
    "# Note: If an existing pipeline has the same name it will be overwritten.\n",
    "deployment_pipeline.upsert(role_arn=role_arn)\n",
    "\n",
    "# Viewing the pipeline definition will all the string variables interpolated may help debug pipeline bugs. It is commented out here due to length.\n",
    "json.loads(deployment_pipeline.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b589851",
   "metadata": {},
   "source": [
    "#### Execute the deployment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f99d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_name = 'my-xgboost-model'\n",
    "execution = deployment_pipeline.start(\n",
    "    parameters = {\n",
    "        'ModelName': deployed_model_name\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4558b9",
   "metadata": {},
   "source": [
    "Check on status of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca47c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba30241b",
   "metadata": {},
   "source": [
    "#### Test the SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c29c1-15a7-45f5-9587-9af776f1d3aa",
   "metadata": {},
   "source": [
    "Let's now send some data to the endpoint and test it is working properly.\n",
    "\n",
    "For this, we first load our test data from Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test set that was used for batch transform\n",
    "fs_group = FeatureGroup(name=test_feature_group_name, sagemaker_session=session)  \n",
    "query = fs_group.athena_query()\n",
    "table = query.table_name\n",
    "query_string = f'SELECT {features_to_select} FROM \"sagemaker_featurestore\".\"{table}\"  ORDER BY record_id'\n",
    "query_results= 'sagemaker-featurestore'\n",
    "output_location = f's3://{bucket}/{query_results}/query_results/'\n",
    "query.run(query_string=query_string, output_location=output_location)\n",
    "query.wait()\n",
    "df = query.as_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88096555-6338-4335-96ed-6e51328306db",
   "metadata": {},
   "source": [
    "Then we query the endpoint once it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df224d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_status = 'None'\n",
    "while response_status != 'InService':\n",
    "    if response_status != 'None':\n",
    "        print(f'Waiting for the endpoint deployment to finish. Current endpoint status: {response_status}')\n",
    "        time.sleep(120) # wait until endpoint is in service\n",
    "    response = sagemaker_client.describe_endpoint(\n",
    "        EndpointName=deployed_model_name+'-endpoint'\n",
    "    )\n",
    "    response_status = response['EndpointStatus']\n",
    "# Attach to the SageMaker endpoint\n",
    "predictor = Predictor(endpoint_name=deployed_model_name+'-endpoint',\n",
    "                      sagemaker_session=session,\n",
    "                      serializer=CSVSerializer(),\n",
    "                      deserializer=CSVDeserializer())\n",
    "\n",
    "# Get a real-time prediction\n",
    "predictor.predict(df.drop(columns=[\"price\"]).to_csv(index=False, header=False))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3aa9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b264e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe947e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073a5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
