{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b55228",
   "metadata": {},
   "source": [
    "# MLOps Manual to Repeatable Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9991b4",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "- [Training pipeline with SageMaker Pipelines](#Training-pipeline-with-SageMaker-Pipelines)\n",
    "    - [Pipeline inputs](#Pipeline-inputs)\n",
    "    - [SageMaker Processing step](#SageMaker-Processing-step)\n",
    "    - [SageMaker Training step](#SageMaker-Training-step)\n",
    "    - [Model evaluation step](#Model-evaluation-step)\n",
    "    - [Register model in Model Registry step](#Register-model-in-Model-Registry-step)\n",
    "    - [Assemble the training pipeline](#Assemble-the-training-pipeline)\n",
    "    - [Execute the training pipeline](#Execute-the-training-pipeline)\n",
    "- [Deployment pipeline with SageMaker Pipelines](#Deployment-pipeline-with-SageMaker-Pipelines)\n",
    "    - [Assemble the deployment pipeline](#Assemble-the-deployment-pipeline)\n",
    "    - [Execute the deployment pipeline](#Execute-the-deployment-pipeline)\n",
    "    - [Test the SageMaker endpoint](#Test-the-SageMaker-endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c955aa1",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a698b834",
   "metadata": {},
   "source": [
    "This is our fourth notebook which will explore the orchestration stage of ML workflow.\n",
    "\n",
    "Here, we will put on the hat of a `DevOps/MLOps Engineer` and perform the task of orchestration which includes building pipeline steps that include all the previous notebooks components into one singular entity. This pipeline entity accomplishes a repeatable and reliable orchestration of each step in the ML workflow.\n",
    "\n",
    "For this task we will be using Amazon SageMaker Pipeline capabilities.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c3c66",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362016c0-d6be-4122-b37c-cbe4a2206788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.109.0)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.20.1)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.5.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.24.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.21.6)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.0.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.2 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.27.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (2.2.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2019.3)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.2->boto3<2.0,>=1.20.21->sagemaker) (1.26.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de783141",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf9398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing imports\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "\n",
    "# SageMaker Pipeline imports\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep, TransformStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "# Other imports\n",
    "import json\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.tuner import IntegerParameter, HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "\n",
    "# To test the endpoint once it's deployed\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer, CSVDeserializer\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "from sagemaker.model_metrics import ModelMetrics, MetricsSource\n",
    "import pandas as pd\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f7796",
   "metadata": {},
   "source": [
    "**Session variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lambda_iam_role(role_name):\n",
    "    iam = boto3.client(\"iam\")\n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps({\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"lambda.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }),\n",
    "            Description='Role for Lambda to call SageMaker'\n",
    "        )\n",
    "\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "        )\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    "            RoleName=role_name\n",
    "        )\n",
    "\n",
    "        return role_arn\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f'Using ARN from existing role: {role_name}')\n",
    "        response = iam.get_role(RoleName=role_name)\n",
    "        print(\"Done\")\n",
    "        return response['Role']['Arn']\n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName = role_name,\n",
    "            AssumeRolePolicyDocument = json.dumps({\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Principal\": {\n",
    "                            \"Service\": \"lambda.amazonaws.com\"\n",
    "                        },\n",
    "                        \"Action\": \"sts:AssumeRole\"\n",
    "                    }\n",
    "                ]\n",
    "            }),\n",
    "            Description='Role for Lambda to call SageMaker'\n",
    "        )\n",
    "\n",
    "        role_arn = response['Role']['Arn']\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "        )\n",
    "\n",
    "        response = iam.attach_role_policy(\n",
    "            PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    "            RoleName=role_name\n",
    "        )\n",
    "        print(\"Done\")\n",
    "\n",
    "        return role_arn\n",
    "\n",
    "    except iam.exceptions.EntityAlreadyExistsException:\n",
    "        print(f'Using ARN from existing role: {role_name}')\n",
    "        response = iam.get_role(RoleName=role_name)\n",
    "        print(\"Done\")\n",
    "        return response['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb244ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful SageMaker variables\n",
    "session = PipelineSession()\n",
    "bucket = session.default_bucket()\n",
    "role_arn= sagemaker.get_execution_role()\n",
    "region = session.boto_region_name\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "aws_account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "lambda_role = create_lambda_iam_role('LambdaSageMakerExecutionRole')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f885a04",
   "metadata": {},
   "source": [
    "## Training pipeline with SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be5471",
   "metadata": {},
   "source": [
    "An Amazon [SageMaker Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) pipeline is a series of interconnected steps that is defined by a JSON pipeline definition. This pipeline definition encodes a pipeline using a directed acyclic graph (DAG). This DAG gives information on the requirements for and relationships between each step of your pipeline. The structure of a pipeline's DAG is determined by the data dependencies between steps. These data dependencies are created when the properties of a step's output are passed as the input to another step. The following image is a pipeline DAG that we'll be creating for our training pipeline:\n",
    "\n",
    "![](./pipeline_scripts/images/sagemaker-pipelines-dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cefc9b",
   "metadata": {},
   "source": [
    "#### Pipeline inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135d550",
   "metadata": {},
   "source": [
    "You can give a pipeline inputs to make it reusable (you'll be able to override these inputs upon executing the pipeline later in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_count = ParameterInteger(\n",
    "    name='ProcessingInstanceCount',\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name='ProcessingInstanceType',\n",
    "    default_value='ml.m5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb440646",
   "metadata": {},
   "source": [
    "#### SageMaker Processing step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961856d",
   "metadata": {},
   "source": [
    "This should look very similar to the SageMaker Training job you did in notebook 2. The only new line of code is the `ProcessingStep` line at the bottom of the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role_arn,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name='preprocess-data',\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "preprocess_dataset_step = ProcessingStep(\n",
    "    name='PreprocessData',\n",
    "    code='./pipeline_scripts/preprocessing.py',\n",
    "    processor=preprocess_data_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=raw_s3,\n",
    "            destination='/opt/ml/processing/input',\n",
    "            s3_data_distribution_type='ShardedByS3Key'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='train',\n",
    "            destination=f'{output_path}/train',\n",
    "            source='/opt/ml/processing/train'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='validation',\n",
    "            destination=f'{output_path}/validation',\n",
    "            source='/opt/ml/processing/validation'\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name='test',\n",
    "            destination=f'{output_path}/test',\n",
    "            source='/opt/ml/processing/test'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a13bfa",
   "metadata": {},
   "source": [
    "#### SageMaker Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa727f",
   "metadata": {},
   "source": [
    "This should look very similar to the SageMaker Training job you did in notebook 2. The only new line of code is the `TrainingStep` line at the bottom of the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65af28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned hyperparameters\n",
    "hyperparameters = {\n",
    "    \"max_depth\": \"7\",\n",
    "    \"gamma\": \"2\",\n",
    "    \"alpha\": \"375\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"50\",\n",
    "    \"verbosity\": \"2\",\n",
    "    \"eval_metric\": \"mse\"\n",
    "}\n",
    "\n",
    "train_instance_type = 'ml.c5.xlarge'\n",
    "\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "#xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.5-1\")\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=xgboost_container, \n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role_arn,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.2xlarge', \n",
    "    volume_size=5, # 5 GB \n",
    ")\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name='TrainModel',\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=preprocess_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'train'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        ),\n",
    "        'validation': TrainingInput(\n",
    "            s3_data=preprocess_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'validation'\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcdd83e",
   "metadata": {},
   "source": [
    "#### Model evaluation step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abb56a",
   "metadata": {},
   "source": [
    "After the training step in our pipeline, we'll want to then evaluate our model's performance. To do that, we can create a SageMaker Processing Step and pass in some code to do the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd97e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_processor = ScriptProcessor(\n",
    "    image_uri=xgboost_container,\n",
    "    command=[\"python3\"],\n",
    "    role=role_arn,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name='evaluation',\n",
    "    sagemaker_session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where we'll store the model evaluation results so\n",
    "# that other steps can access those results\n",
    "evaluation_report = PropertyFile(\n",
    "    name='EvaluationReport',\n",
    "    output_name='evaluation',\n",
    "    path='evaluation.json',\n",
    ")\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name='EvaluateModel',\n",
    "    processor=evaluation_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/model',\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=preprocess_dataset_step.properties.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri,\n",
    "            destination='/opt/ml/processing/test',\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='evaluation', source='/opt/ml/processing/evaluation'\n",
    "        ),\n",
    "    ],\n",
    "    code='./pipeline_scripts/evaluation.py',\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd32643",
   "metadata": {},
   "source": [
    "#### Register model in Model Registry step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b43c83",
   "metadata": {},
   "source": [
    "Once we've evaluated the model's peformance, we'll want to register the model in a Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri='{}/evaluation.json'.format(\n",
    "            evaluation_step.arguments['ProcessingOutputConfig']['Outputs'][0]['S3Output'][\n",
    "                'S3Uri'\n",
    "            ]\n",
    "        ),\n",
    "        content_type='application/json',\n",
    "    )\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    image_uri=estimator.training_image_uri(),\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    entry_point=estimator.entry_point,\n",
    "    role=role_arn,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "model_registry_args = model.register(\n",
    "    content_types=['text/csv'],\n",
    "    response_types=['application/json'],\n",
    "    inference_instances=['ml.t2.medium', 'ml.m5.xlarge'],\n",
    "    transform_instances=['ml.m5.xlarge'],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status='PendingManualApproval',\n",
    "    model_metrics=model_metrics\n",
    ")\n",
    "\n",
    "register_step = ModelStep(\n",
    "    name='RegisterModel',\n",
    "    step_args=model_registry_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb436e40",
   "metadata": {},
   "source": [
    "But we'll only want to register the model if its performance meets a predefined threshold that we set. So let's create a Condition Step that says if our model's MSE values is less than 80000000.0, then we'll registery the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a45852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition step for evaluating model quality and branching execution\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path='regression_metrics.mse.value',\n",
    "    ),\n",
    "    right=80000000.0,\n",
    ")\n",
    "condition_step = ConditionStep(\n",
    "    name='CheckEvaluation',\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[register_step],\n",
    "    else_steps=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae7857",
   "metadata": {},
   "source": [
    "#### Assemble the training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d737ad",
   "metadata": {},
   "source": [
    "Though easier to reason with, the parameters and steps don't need to be in order. The pipeline DAG will parse it out properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = 'synthetic-housing-training-pipeline-{}'.format(strftime('%d-%H-%M-%S', gmtime()))\n",
    "pipeline_name = 'synthetic-housing-training-pipeline'\n",
    "step_list = [preprocess_dataset_step,\n",
    "             training_step,\n",
    "             evaluation_step,\n",
    "             condition_step]\n",
    "\n",
    "training_pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        processing_instance_type\n",
    "    ],\n",
    "    steps=step_list\n",
    ")\n",
    "\n",
    "# Note: If an existing pipeline has the same name it will be overwritten.\n",
    "training_pipeline.upsert(role_arn=role_arn)\n",
    "\n",
    "# Viewing the pipeline definition will all the string variables interpolated may help debug pipeline bugs. It is commented out here due to length.\n",
    "#json.loads(training_pipeline.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e5187",
   "metadata": {},
   "source": [
    "#### Execute the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = training_pipeline.start(\n",
    "    parameters = {\n",
    "        'ProcessingInstanceType': 'ml.m5.large'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d7ff1",
   "metadata": {},
   "source": [
    "Check on status of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8bc68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84fabe",
   "metadata": {},
   "source": [
    "## Deployment pipeline with SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9770d04",
   "metadata": {},
   "source": [
    "Now let's create a separate pipeline that will take the model that was registered in Model Registry and deploy it as a SageMaker hosted endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0f831",
   "metadata": {},
   "source": [
    "First we'll specify the input parameters to our deployment pipeline so that we can reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586cb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ParameterString(\n",
    "    name='ModelName',\n",
    "    default_value='my-awesome-model'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e162f",
   "metadata": {},
   "source": [
    "Next, we'll create a Lambda function that will pull the specified model (or latest model) from the Model Registry and deploy as a Sagemaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_name = 'sagemaker-pipelines-deploy-model'\n",
    "\n",
    "lambda_function = Lambda(\n",
    "    function_name=lambda_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    script='./pipeline_scripts/lambda_deploy.py',\n",
    "    handler='lambda_deploy.lambda_handler',\n",
    "    timeout=600,\n",
    "    memory_size=3000,\n",
    ")\n",
    "\n",
    "try:\n",
    "    lambda_function_response = lambda_function.create()\n",
    "    lambda_function_arn = lambda_function_response['FunctionArn']\n",
    "    print(f'Lambda function arn: {lambda_function_arn}')\n",
    "except:\n",
    "    print('Lambda function already exists!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b514b2cd",
   "metadata": {},
   "source": [
    "Now we'll create a Lambda step for our pipeline and associate it with the new Lambda function we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary retured by the Lambda function is captured by LambdaOutput, each key in the dictionary corresponds to a\n",
    "# LambdaOutput\n",
    "\n",
    "output_param_1 = LambdaOutput(output_name='statusCode', output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_2 = LambdaOutput(output_name='body', output_type=LambdaOutputTypeEnum.String)\n",
    "\n",
    "deploy_lambda_step = LambdaStep(\n",
    "    name='LambdaStepDeploy',\n",
    "    lambda_func=lambda_function,\n",
    "    inputs={\n",
    "        'region': region,\n",
    "        'aws_account_id': aws_account_id,\n",
    "        'model_package_group_name': model_package_group_name,\n",
    "        'model_name': model_name,\n",
    "        'instance_count': 1,\n",
    "        'role_arn': role_arn\n",
    "    },\n",
    "    outputs=[\n",
    "        output_param_1, \n",
    "        output_param_2\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d029f8c",
   "metadata": {},
   "source": [
    "Excellent, now we just need to assemble the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc014f56",
   "metadata": {},
   "source": [
    "#### Assemble the deployment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_name = 'synthetic-housing-deployment-pipeline-{}'.format(strftime('%d-%H-%M-%S', gmtime()))\n",
    "pipeline_name = 'synthetic-housing-deployment-pipeline'\n",
    "step_list = [deploy_lambda_step]\n",
    "\n",
    "deployment_pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        model_name\n",
    "    ],\n",
    "    steps=step_list\n",
    ")\n",
    "\n",
    "# Note: If an existing pipeline has the same name it will be overwritten.\n",
    "deployment_pipeline.upsert(role_arn=role_arn)\n",
    "\n",
    "# Viewing the pipeline definition will all the string variables interpolated may help debug pipeline bugs. It is commented out here due to length.\n",
    "json.loads(deployment_pipeline.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b589851",
   "metadata": {},
   "source": [
    "#### Execute the deployment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f99d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_name = 'my-xgboost-model'\n",
    "execution = deployment_pipeline.start(\n",
    "    parameters = {\n",
    "        'ModelName': deployed_model_name\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4558b9",
   "metadata": {},
   "source": [
    "Check on status of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca47c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba30241b",
   "metadata": {},
   "source": [
    "#### Test the SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c29c1-15a7-45f5-9587-9af776f1d3aa",
   "metadata": {},
   "source": [
    "Let's now send some data to the endpoint and test it is working properly.\n",
    "\n",
    "For this, we first load our test data from Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test set that was used for batch transform\n",
    "fs_group = FeatureGroup(name=test_feature_group_name, sagemaker_session=session)  \n",
    "query = fs_group.athena_query()\n",
    "table = query.table_name\n",
    "query_string = f'SELECT {features_to_select} FROM \"sagemaker_featurestore\".\"{table}\"  ORDER BY record_id'\n",
    "query_results= 'sagemaker-featurestore'\n",
    "output_location = f's3://{bucket}/{query_results}/query_results/'\n",
    "query.run(query_string=query_string, output_location=output_location)\n",
    "query.wait()\n",
    "df = query.as_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88096555-6338-4335-96ed-6e51328306db",
   "metadata": {},
   "source": [
    "Then we query the endpoint once it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df224d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_status = 'None'\n",
    "while response_status != 'InService':\n",
    "    if response_status != 'None':\n",
    "        print(f'Waiting for the endpoint deployment to finish. Current endpoint status: {response_status}')\n",
    "        time.sleep(120) # wait until endpoint is in service\n",
    "    response = sagemaker_client.describe_endpoint(\n",
    "        EndpointName=deployed_model_name+'-endpoint'\n",
    "    )\n",
    "    response_status = response['EndpointStatus']\n",
    "# Attach to the SageMaker endpoint\n",
    "predictor = Predictor(endpoint_name=deployed_model_name+'-endpoint',\n",
    "                      sagemaker_session=session,\n",
    "                      serializer=CSVSerializer(),\n",
    "                      deserializer=CSVDeserializer())\n",
    "\n",
    "# Get a real-time prediction\n",
    "predictor.predict(df.drop(columns=[\"price\"]).to_csv(index=False, header=False))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3aa9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b264e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe947e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073a5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
